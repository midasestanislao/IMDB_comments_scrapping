{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Authors:** \n",
    "### **- Héctor A. Robles P.**\n",
    "###  **- Gregory G. Richarte P.**\n",
    "###  **- Brandon S. Martinez L.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import chromedriver_autoinstaller\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from joblib import Parallel, delayed \n",
    "from concurrent import futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datos(matriz, y_test, precision):\n",
    "     # Definición de datos del modelo\n",
    "    datos = {\n",
    "        'Precision Global': [],\n",
    "        'Error Global': [],\n",
    "        'Precision Positiva': [],\n",
    "        'Precision Negativa': [],\n",
    "        'Falsos Positivos (FP)': [],\n",
    "        'Falsos Negativos(FN)': [],\n",
    "        'Asertividad Positiva(AP)': [],\n",
    "        'Asertividad Negativa(AN)': [],\n",
    "        'Precision del modelo': []\n",
    "    }\n",
    "    \n",
    "    # Almacenamiento de los valores en el diccionario datos\n",
    "    datos['Precision Global'].append((matriz[0, 0] + matriz[1, 1]) / len(y_test))\n",
    "    datos['Error Global'].append((matriz[0, 1] + matriz[1, 0]) / len(y_test))\n",
    "    datos['Precision Positiva'].append(matriz[1, 1] / (matriz[1, 1] + matriz[0, 1]))\n",
    "    datos['Precision Negativa'].append(matriz[0, 0] / (matriz[0, 0] + matriz[1, 0]))\n",
    "    datos['Falsos Positivos (FP)'].append(matriz[0, 1] / (matriz[0, 1] + matriz[0, 0]))\n",
    "    datos['Falsos Negativos(FN)'].append(matriz[1, 0] / (matriz[1, 0] + matriz[1, 1]))\n",
    "    datos['Asertividad Positiva(AP)'].append(matriz[1, 1] / (matriz[1, 1] + matriz[0, 1]))\n",
    "    datos['Asertividad Negativa(AN)'].append(matriz[0, 0] / (matriz[0, 0] + matriz[1, 0]))\n",
    "    datos['Precision del modelo'].append(precision)\n",
    "    \n",
    "    return datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar pagina\n",
    "movie_list = [\n",
    "  \"https://www.imdb.com/title/tt0149460/reviews\",   # Futurama\n",
    "  \"https://www.imdb.com/title/tt0386676/reviews\",   # The Office\n",
    "  \"https://www.imdb.com/title/tt0903747/reviews\"   # Breaking Bad\n",
    "  \"https://www.imdb.com/title/tt1520211/reviews\",   # The Walking Dead\n",
    "  \"https://www.imdb.com/title/tt4574334/reviews\",   # Stranger Things\n",
    "  \"https://www.imdb.com/title/tt1190634/reviews\"   # The Boys\n",
    "  \"https://www.imdb.com/title/tt0412142/reviews\",   # House M.D.\n",
    "  \"https://www.imdb.com/title/tt0182576/reviews\",   # Family Guy\n",
    "  \"https://www.imdb.com/title/tt0068646/reviews\",   # The Godfather\n",
    "  \"https://www.imdb.com/title/tt2861424/reviews\",   # Rick and Morty\n",
    "  \"https://www.imdb.com/title/tt0121955/reviews\",   # South Park\n",
    "  \"https://www.imdb.com/title/tt2521884/reviews\",   # Drawn Together\n",
    "  \"https://www.imdb.com/title/tt0898266/reviews\",   # The Big Bang Theory\n",
    "  \"https://www.imdb.com/title/tt2085059/reviews\",   # Black Mirror\n",
    "  \"https://www.imdb.com/title/tt0098844/reviews\",   # Law & Order\n",
    "  \"https://www.imdb.com/title/tt0212671/reviews\",   # Malcolm in the Middle\n",
    "  \"https://www.imdb.com/title/tt0460649/reviews\",   # How I Met Your Mother\n",
    "  \"https://www.imdb.com/title/tt7366338/reviews\",   # Chernobyl\n",
    "  \"https://www.imdb.com/title/tt0141842/reviews\",   # The Sopranos\n",
    "  \"https://www.imdb.com/title/tt1099212/reviews\",   # Dragonball Evolution\n",
    "  \"https://www.imdb.com/title/tt0413573/reviews\",   # Grey's Anatomy\n",
    "  \"https://www.imdb.com/title/tt1506229/reviews\",   # The Beatles: Get Back\n",
    "  \"https://www.imdb.com/title/tt1865718/reviews\",   # Gravity Falls\n",
    "  \"https://www.imdb.com/title/tt0096697/reviews\",   # The Simpsons\n",
    "  \"https://www.imdb.com/title/tt0285403/reviews\" ,  # The Last of Us\n",
    "  \"https://www.imdb.com/title/tt0068468/reviews\"    # El Chavo del Ocho\n",
    "]\n",
    "\n",
    "def get_serie(movie):\n",
    "    data = {\n",
    "        'comment':[],\n",
    "        'score': [],\n",
    "        'rate': []\n",
    "    }\n",
    "\n",
    "    chromedriver_autoinstaller.install()\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    driver.get(movie)\n",
    "    time.sleep(2)\n",
    "    for _ in range(20):\n",
    "        try:\n",
    "            load_more_button = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.ID, \"load-more-trigger\"))\n",
    "            )\n",
    "\n",
    "            load_more_button.click()\n",
    "\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    updated_html_content = driver.page_source\n",
    "\n",
    "    body = BeautifulSoup(updated_html_content, 'html.parser')\n",
    "\n",
    "    opinions = body.find_all('div', class_='lister-item')\n",
    "\n",
    "    for o in opinions:\n",
    "        try:\n",
    "            score = int(list(o.find('span', class_='rating-other-user-rating').children)[3].text)\n",
    "            comment = o.find('div', class_='text').text\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        data['comment'].append(comment)\n",
    "        data['score'].append(score)\n",
    "        if(score >= 7):\n",
    "            data['rate'].append('1')\n",
    "        else:\n",
    "            data['rate'].append('0')\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return data\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "with futures.ThreadPoolExecutor() as executor: # default/optimized number of threads\n",
    "  titles = list(executor.map(get_serie, movie_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {\n",
    "    'comment':[],\n",
    "    'score': [],\n",
    "    'rate': []\n",
    "}\n",
    "\n",
    "for t in titles:\n",
    "    all_data['comment'] += t['comment']\n",
    "    all_data['score'] += t['score']\n",
    "    all_data['rate'] += t['rate']\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "df\n",
    "data1 = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Los datos son usados para CSV para su procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_data = df.to_csv('dataset_1.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "from string import punctuation\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/scavenger/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/scavenger/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "      <th>rate</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Compound</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This show, well lets say it's probably one of ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.9786</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was so excited when I heard about new season...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.9848</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The television series Futurama is nothing shor...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.9829</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Now that is not to say the Simpsons is bad or ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This show is so brilliant you don't deserve to...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.7065</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  score rate  Positive  \\\n",
       "0  This show, well lets say it's probably one of ...      8    1     0.201   \n",
       "1  I was so excited when I heard about new season...      9    1     0.106   \n",
       "2  The television series Futurama is nothing shor...      8    1     0.131   \n",
       "3  Now that is not to say the Simpsons is bad or ...     10    1     0.414   \n",
       "4  This show is so brilliant you don't deserve to...     10    1     0.327   \n",
       "\n",
       "   Negative  Compound Sentiment  \n",
       "0     0.012    0.9786  Positive  \n",
       "1     0.202   -0.9848  Negative  \n",
       "2     0.051    0.9829  Positive  \n",
       "3     0.021    0.9990  Positive  \n",
       "4     0.000    0.7065  Positive  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "sentiments = SentimentIntensityAnalyzer()\n",
    "df[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in df['comment']]\n",
    "df[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in df['comment']]\n",
    "# df[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in df['comment']]\n",
    "df[\"Compound\"] = [sentiments.polarity_scores(i)[\"compound\"] for i in df['comment']]\n",
    "score = df[\"Compound\"].values\n",
    "sentiment = []\n",
    "for i in score:\n",
    "    if i >= 0.05:\n",
    "        sentiment.append('Positive')\n",
    "    elif i < 0.05:\n",
    "        sentiment.append('Negative')\n",
    "    # else:\n",
    "    #     sentiment.append('Neutral')\n",
    "\n",
    "df[\"Sentiment\"] = sentiment\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "      <th>rate</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This show, well lets say it's probably one of ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was so excited when I heard about new season...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The television series Futurama is nothing shor...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Now that is not to say the Simpsons is bad or ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This show is so brilliant you don't deserve to...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  score rate Sentiment\n",
       "0  This show, well lets say it's probably one of ...      8    1  Positive\n",
       "1  I was so excited when I heard about new season...      9    1  Negative\n",
       "2  The television series Futurama is nothing shor...      8    1  Positive\n",
       "3  Now that is not to say the Simpsons is bad or ...     10    1  Positive\n",
       "4  This show is so brilliant you don't deserve to...     10    1  Positive"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = df.drop(['Positive', 'Negative', 'Compound'], axis=1)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer() \n",
    "snowball_stemer = SnowballStemmer(language=\"english\")\n",
    "lzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text):   \n",
    "    # convert text into lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove new line characters in text\n",
    "    text = re.sub(r'\\n',' ', text)\n",
    "    \n",
    "    # remove punctuations from text\n",
    "    text = re.sub('[%s]' % re.escape(punctuation), \"\", text)\n",
    "    \n",
    "    # remove references and hashtags from text\n",
    "    text = re.sub(\"^a-zA-Z0-9$,.\", \"\", text)\n",
    "    \n",
    "    # remove multiple spaces from text\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    \n",
    "    # remove special characters from text\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "    text = ' '.join([word for word in word_tokenize(text) if word not in stop_words])\n",
    "    \n",
    "    # lemmatizer using WordNetLemmatizer from nltk package\n",
    "    text=' '.join([lzr.lemmatize(word) for word in word_tokenize(text)])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/scavenger/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "data_copy = data2.copy()\n",
    "data_copy.comment = data_copy.comment.apply(lambda text: text_processing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "data_copy['Sentiment'] = le.fit_transform(data_copy['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>show well let say probably one original show s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>excited heard new season watched watched next ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>television series futurama nothing short brill...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>say simpson bad anything really like jumped sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>show brilliant dont deserve watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Sentiment\n",
       "0  show well let say probably one original show s...          1\n",
       "1  excited heard new season watched watched next ...          0\n",
       "2  television series futurama nothing short brill...          1\n",
       "3  say simpson bad anything really like jumped sh...          1\n",
       "4                  show brilliant dont deserve watch          1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = {\n",
    "    'Sentence':data_copy.comment,\n",
    "    'Sentiment':data_copy['Sentiment']\n",
    "}\n",
    "\n",
    "processed_data = pd.DataFrame(processed_data)\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "1    5723\n",
       "0    1336\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative = processed_data[(processed_data['Sentiment']==0)]\n",
    "df_positive = processed_data[(processed_data['Sentiment']==1)]\n",
    "\n",
    "final_data = pd.concat([df_negative, df_positive])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para convertir en CSV\n",
    "csv_data = final_data.to_csv('dataset_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "1    5723\n",
       "0    1336\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['excited heard new season watched watched next episode next fourth last one watch know writing bad stop ruin whole show new material like bad ripoff original show still dvd occasionally watchi excited heard new season watched watched next episode next fourth last one watch know writing bad stop ruin whole show new material like bad ripoff original show still dvd occasionally watchi excited heard new season watched watched next episode next fourth last one watch know writing bad stop ruin whole show new material like bad ripoff original show still dvd occasionally watch',\n",
       " 'first three season really interesting funny long time laugh many episode however fourth sixth everything changed joke became somehow flat banal toilet humor overly stupid futuramastyle absurdly funny style simply stupid like episode robot took gun stupidly shoot bullet air half minute episode complains gun empty useless joke happened joke sake joke rudely thrown episode hope viewer take style series result season completely raw funny since main highlight series mottled story longer aroused much interest losing charm last season better good enough like first three still better fourth sixth one however overall impression series still remained disappointment',\n",
       " 'amazing show unfortunately hasnt gotten attention rightfully deserves even weaker season thought intelligent weaker season lot long running animated show',\n",
       " 'enjoy watching episode writing feel lazywhile simpson may lewd joke subtle adult theme sex plot lot futurama episode dont watch childrenthe show often relies stereotype spoofing thingsevery episode kinda circular mean must reset normalcy tv time show began people couldnt binge whole season like nonetheless put together creates somewhat inconsistent storylinebender interesting sometimes life many year seems immortal sometimes appears close death hour without alcoholits little weird much hate mean zoidberg make sadwomen judge get half vote seriously female robot highly sexualized male character typically respect female onesleela fry weird relationship fry pining leela sometimes seems like theyre made together next episode leela hate feel like lazy writingthere lot sexual assault show downplayed made jokei dont understand keep bender around literally villain half episodesthere handful great episode really try explore scientific social concept many memorablei guess sum deep want think much one pretty good chill withbut kid',\n",
       " 'one favorit show time went 10 1 new season horribly written script show lost comedic value 100 doesnt even feel like show character sound almost feel robotic characteri disappointed show seemingly every tv showmovie since world decided turn upside cough 2019every episode get worse worse attempt bring current event fail miserably entertainment value equivalent watching paint dry punched ball']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "for sentence in final_data['Sentence']:\n",
    "    corpus.append(sentence)\n",
    "corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = final_data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Navie Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusion:\n",
      "[[295 102]\n",
      " [781 940]]\n",
      "Precision del modelo:\n",
      "0.9021113243761996\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Definir X_train, X_test, y_train, y_test usando train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Definición del algoritmo Naive Bayes\n",
    "algoritmo = GaussianNB()\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realización de la predicción\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "\n",
    "# Cálculo de la matriz de confusión\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusion:')\n",
    "print(matriz)\n",
    "\n",
    "# Cálculo de la precisión del modelo\n",
    "nb_precision = precision_score(y_test, y_pred)\n",
    "print('Precision del modelo:')\n",
    "print(nb_precision)\n",
    "\n",
    "# Convertir el diccionario en un DataFrame\n",
    "nb_datos = pd.DataFrame(get_datos(matriz, y_test, nb_precision))\n",
    "indice_1 = pd.DataFrame(get_datos(matriz, y_test, nb_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo NNBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusion:\n",
      "[[ 224  173]\n",
      " [ 119 1602]]\n",
      "Precision del modelo:\n",
      "0.9025352112676056\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Definir X_train, X_test, y_train, y_test usando train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Definición del algoritmo MLP\n",
    "algoritmo = MLPClassifier()\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realización de la predicción\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "\n",
    "# Cálculo de la matriz de confusión\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusion:')\n",
    "print(matriz)\n",
    "\n",
    "# Cálculo de la precisión del modelo\n",
    "mlp_precision = precision_score(y_test, y_pred)\n",
    "print('Precision del modelo:')\n",
    "print(mlp_precision)\n",
    "\n",
    "# Convertir el diccionario en un DataFrame\n",
    "\n",
    "mlp_datos = pd.DataFrame(get_datos(matriz, y_test, mlp_precision))\n",
    "indice_2 = pd.DataFrame(get_datos(matriz, y_test, mlp_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MAQUINAS DE VECTORES DE SOPORTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusion:\n",
      "[[ 222  175]\n",
      " [ 163 1558]]\n",
      "Precision del modelo:\n",
      "0.8990190421234853\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "\n",
    "# Definición del algoritmo SVM\n",
    "algoritmo = SVC(kernel='linear')\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "algoritmo.fit(X_train, y_train)\n",
    "\n",
    "# Realización de la predicción\n",
    "y_pred = algoritmo.predict(X_test)\n",
    "\n",
    "# Cálculo de la matriz de confusión\n",
    "matriz = confusion_matrix(y_test, y_pred)\n",
    "print('Matriz de Confusion:')\n",
    "print(matriz)\n",
    "\n",
    "# Cálculo de la precisión del modelo\n",
    "svm_precision = precision_score(y_test, y_pred)\n",
    "print('Precision del modelo:')\n",
    "print(svm_precision)\n",
    "\n",
    "# Convertir el diccionario en un DataFrame\n",
    "svm_datos = pd.DataFrame(get_datos(matriz, y_test, svm_precision))\n",
    "indice_3 = pd.DataFrame(get_datos(matriz, y_test, svm_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla de comparacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Precision Positiva  Precision Negativa  Precision Global  \\\n",
      "Naive Bayes            0.902111            0.274164          0.583097   \n",
      "MLP                    0.902535            0.653061          0.862134   \n",
      "SVM                    0.899019            0.576623          0.840415   \n",
      "\n",
      "             Error Global  Falsos Positivos (FP)  Falsos Negativos(FN)  \\\n",
      "Naive Bayes      0.416903               0.256927              0.453806   \n",
      "MLP              0.137866               0.435768              0.069146   \n",
      "SVM              0.159585               0.440806              0.094712   \n",
      "\n",
      "             Asertividad Positiva(AP)  Asertividad Negativa(AN)  \\\n",
      "Naive Bayes                  0.902111                  0.274164   \n",
      "MLP                          0.902535                  0.653061   \n",
      "SVM                          0.899019                  0.576623   \n",
      "\n",
      "             Precision del modelo  \n",
      "Naive Bayes              0.902111  \n",
      "MLP                      0.902535  \n",
      "SVM                      0.899019  \n"
     ]
    }
   ],
   "source": [
    "# Convertir los datos en DataFrames\n",
    "indice_datos = {\n",
    "    'Precision Positiva': [nb_datos['Precision Positiva'][0], mlp_datos['Precision Positiva'][0], svm_datos['Precision Positiva'][0]],\n",
    "    'Precision Negativa': [nb_datos['Precision Negativa'][0], mlp_datos['Precision Negativa'][0], svm_datos['Precision Negativa'][0]],\n",
    "    'Precision Global': [nb_datos['Precision Global'][0], mlp_datos['Precision Global'][0], svm_datos['Precision Global'][0]],\n",
    "    'Error Global': [nb_datos['Error Global'][0], mlp_datos['Error Global'][0], svm_datos['Error Global'][0]],\n",
    "    'Falsos Positivos (FP)': [nb_datos['Falsos Positivos (FP)'][0], mlp_datos['Falsos Positivos (FP)'][0], svm_datos['Falsos Positivos (FP)'][0]],\n",
    "    'Falsos Negativos(FN)': [nb_datos['Falsos Negativos(FN)'][0], mlp_datos['Falsos Negativos(FN)'][0], svm_datos['Falsos Negativos(FN)'][0]],\n",
    "    'Asertividad Positiva(AP)': [nb_datos['Asertividad Positiva(AP)'][0], mlp_datos['Asertividad Positiva(AP)'][0], svm_datos['Asertividad Positiva(AP)'][0]],\n",
    "    'Asertividad Negativa(AN)': [nb_datos['Asertividad Negativa(AN)'][0], mlp_datos['Asertividad Negativa(AN)'][0], svm_datos['Asertividad Negativa(AN)'][0]],\n",
    "    'Precision del modelo': [nb_precision, mlp_precision, svm_precision]\n",
    "}\n",
    "\n",
    "indice_ids = ['Naive Bayes', 'MLP', 'SVM']\n",
    "indice_df = pd.DataFrame(indice_datos, index=indice_ids)\n",
    "\n",
    "print(indice_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluciones.\n",
    "### Bibliografia.\n",
    "### URL: SCIKIT-LEARN (Machine Learning in Python 2022). https://scikit-learn.org/stable/¶"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
